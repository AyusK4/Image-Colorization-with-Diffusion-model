{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e347c47",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df479b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -q diffusers scikit-image\n",
    "\n",
    "print(\"âœ… Packages ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0a16c",
   "metadata": {},
   "source": [
    "## ðŸ“š Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import lab2rgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"âœ… Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c61f6d",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558df637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PATHS \n",
    "# ============================================================\n",
    "CHECKPOINT_PATH = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Image colorization\\\\checkpoint_epoch_26.pth\" \n",
    "VAL_H5_PATH = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Image colorization\\\\val.h5\" \n",
    "OUTPUT_DIR = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Image colorization\\\\inference_results\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# MODEL PARAMETERS (must match training)\n",
    "# ============================================================\n",
    "IMG_SIZE = 128\n",
    "NUM_INFERENCE_STEPS = 50  # Number of denoising steps\n",
    "NUM_SAMPLES = 50  # Number of images to colorize\n",
    "\n",
    "# Diffusion parameters (from training)\n",
    "NUM_TRAIN_TIMESTEPS = 1000\n",
    "BETA_START = 0.0001\n",
    "BETA_END = 0.02\n",
    "BETA_SCHEDULE = \"linear\"\n",
    "\n",
    "print(\"âœ… Configuration set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53354a",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 4: Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorDiffusionModel(nn.Module):\n",
    "    \"\"\"Conditional diffusion model for colorization.\"\"\"\n",
    "    \n",
    "    def __init__(self, unet, noise_scheduler):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "\n",
    "    def forward(self, L, AB, timesteps):\n",
    "        \"\"\"Forward pass: predict noise from L + noisy_AB\"\"\"\n",
    "        model_input = torch.cat([L, AB], dim=1)  # (B, 3, 128, 128)\n",
    "        predicted_noise = self.unet(model_input, timesteps).sample\n",
    "        return predicted_noise\n",
    "\n",
    "    def add_noise(self, AB, noise, timesteps):\n",
    "        \"\"\"Add noise to AB channels\"\"\"\n",
    "        return self.noise_scheduler.add_noise(AB, noise, timesteps)\n",
    "\n",
    "print(\"âœ… Model class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa8a95",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Step 5: Load Model from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model from checkpoint...\")\n",
    "\n",
    "# Initialize UNet (same architecture as training)\n",
    "unet = UNet2DModel(\n",
    "    sample_size=IMG_SIZE,\n",
    "    in_channels=3,\n",
    "    out_channels=2,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(96, 192, 384, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    "    attention_head_dim=8,\n",
    ")\n",
    "\n",
    "# Initialize noise scheduler\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=NUM_TRAIN_TIMESTEPS,\n",
    "    beta_start=BETA_START,\n",
    "    beta_end=BETA_END,\n",
    "    beta_schedule=BETA_SCHEDULE,\n",
    "    prediction_type=\"epsilon\",\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = ColorDiffusionModel(unet, noise_scheduler)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Print checkpoint info\n",
    "print(f\"âœ… Loaded checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "print(f\"   Train loss: {checkpoint['train_loss']:.4f}\")\n",
    "print(f\"   Val loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"   Model parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc101b",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 6: Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading validation data from {VAL_H5_PATH}...\")\n",
    "\n",
    "# Load validation images\n",
    "with h5py.File(VAL_H5_PATH, 'r') as f:\n",
    "    # Load first NUM_SAMPLES images\n",
    "    val_images = f['images'][:NUM_SAMPLES].astype(np.float32)\n",
    "\n",
    "print(f\"âœ… Loaded {len(val_images)} validation images\")\n",
    "print(f\"   Shape: {val_images.shape}\")\n",
    "print(f\"   Dtype: {val_images.dtype}\")\n",
    "\n",
    "# Split into L and AB channels\n",
    "L_images = val_images[:, :, :, 0:1]  # (N, 128, 128, 1)\n",
    "AB_images = val_images[:, :, :, 1:3]  # (N, 128, 128, 2)\n",
    "\n",
    "# Convert to PyTorch tensors: (N, H, W, C) -> (N, C, H, W)\n",
    "L_batch = torch.from_numpy(L_images).permute(0, 3, 1, 2).to(device)  # (N, 1, 128, 128)\n",
    "AB_batch_gt = torch.from_numpy(AB_images).permute(0, 3, 1, 2).to(device)  # (N, 2, 128, 128)\n",
    "\n",
    "print(f\"âœ… Prepared tensors for inference\")\n",
    "print(f\"   L shape: {L_batch.shape}\")\n",
    "print(f\"   AB shape: {AB_batch_gt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e395851",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Step 7: Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a243ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running inference with {NUM_INFERENCE_STEPS} denoising steps...\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def colorize_images(model, L_batch, num_steps=50):\n",
    "    \"\"\"\n",
    "    Colorize grayscale images using diffusion model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ColorDiffusionModel\n",
    "        L_batch: Grayscale images (N, 1, H, W) in range [0, 1]\n",
    "        num_steps: Number of denoising steps\n",
    "    \n",
    "    Returns:\n",
    "        predicted_AB: Colorized AB channels (N, 2, H, W) in range [-1, 1]\n",
    "    \"\"\"\n",
    "    batch_size = L_batch.shape[0]\n",
    "    \n",
    "    # Set timesteps for inference\n",
    "    model.noise_scheduler.set_timesteps(num_steps)\n",
    "    \n",
    "    # Start from random noise\n",
    "    predicted_AB = torch.randn(batch_size, 2, IMG_SIZE, IMG_SIZE, device=L_batch.device)\n",
    "    \n",
    "    # Denoise step by step\n",
    "    for t in tqdm(model.noise_scheduler.timesteps, desc=\"Denoising\"):\n",
    "        # Create timestep tensor\n",
    "        timesteps = torch.full((batch_size,), t, device=L_batch.device, dtype=torch.long)\n",
    "        \n",
    "        # Predict noise\n",
    "        pred_noise = model(L_batch, predicted_AB, timesteps)\n",
    "        \n",
    "        # Denoise\n",
    "        predicted_AB = model.noise_scheduler.step(pred_noise, t, predicted_AB).prev_sample\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    predicted_AB = predicted_AB.clamp(-1.0, 1.0)\n",
    "    \n",
    "    return predicted_AB\n",
    "\n",
    "# Run inference\n",
    "predicted_AB = colorize_images(model, L_batch, num_steps=NUM_INFERENCE_STEPS)\n",
    "\n",
    "print(f\"âœ… Inference complete!\")\n",
    "print(f\"   Predicted AB range: [{predicted_AB.min().item():.3f}, {predicted_AB.max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62c759",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 8: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedfdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to CPU for visualization\n",
    "L_batch_cpu = L_batch.cpu()\n",
    "AB_batch_gt_cpu = AB_batch_gt.cpu()\n",
    "predicted_AB_cpu = predicted_AB.cpu()\n",
    "\n",
    "# Function to convert LAB to RGB\n",
    "def lab_to_rgb(L, AB):\n",
    "    \"\"\"\n",
    "    Convert LAB tensors to RGB numpy array.\n",
    "    \n",
    "    Args:\n",
    "        L: (C, H, W) tensor in range [0, 1]\n",
    "        AB: (C, H, W) tensor in range [-1, 1]\n",
    "    \n",
    "    Returns:\n",
    "        RGB image (H, W, 3) in range [0, 1]\n",
    "    \"\"\"\n",
    "    # Concatenate L and AB\n",
    "    lab = torch.cat([L, AB], dim=0)  # (3, H, W)\n",
    "    lab = lab.permute(1, 2, 0).numpy()  # (H, W, 3)\n",
    "    \n",
    "    # Denormalize: L: [0,1]->[ 0,100], AB: [-1,1]->[-128,128]\n",
    "    lab = np.stack([\n",
    "        lab[:, :, 0] * 100,\n",
    "        lab[:, :, 1] * 128,\n",
    "        lab[:, :, 2] * 128\n",
    "    ], axis=-1)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    rgb = lab2rgb(lab)\n",
    "    return np.clip(rgb, 0, 1)\n",
    "\n",
    "# Display results\n",
    "num_display = min(8, NUM_SAMPLES)\n",
    "fig, axes = plt.subplots(3, num_display, figsize=(num_display * 3, 9))\n",
    "\n",
    "for i in range(num_display):\n",
    "    # Grayscale input\n",
    "    L_gray = L_batch_cpu[i, 0].numpy()\n",
    "    axes[0, i].imshow(L_gray, cmap='gray')\n",
    "    axes[0, i].set_title('Input (Grayscale)', fontweight='bold')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Ground truth\n",
    "    rgb_gt = lab_to_rgb(L_batch_cpu[i], AB_batch_gt_cpu[i])\n",
    "    axes[1, i].imshow(rgb_gt)\n",
    "    axes[1, i].set_title('Ground Truth', fontweight='bold')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "    # Prediction\n",
    "    rgb_pred = lab_to_rgb(L_batch_cpu[i], predicted_AB_cpu[i])\n",
    "    axes[2, i].imshow(rgb_pred)\n",
    "    axes[2, i].set_title('Prediction', fontweight='bold')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(OUTPUT_DIR, 'inference_results.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"âœ… Saved results to {output_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff0f38",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 9: Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9be5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing metrics...\")\n",
    "\n",
    "# Compute statistics\n",
    "with torch.no_grad():\n",
    "    # Ground truth statistics\n",
    "    gt_mean_a = AB_batch_gt_cpu[:, 0].mean().item()\n",
    "    gt_mean_b = AB_batch_gt_cpu[:, 1].mean().item()\n",
    "    gt_std_a = AB_batch_gt_cpu[:, 0].std().item()\n",
    "    gt_std_b = AB_batch_gt_cpu[:, 1].std().item()\n",
    "    \n",
    "    # Prediction statistics\n",
    "    pred_mean_a = predicted_AB_cpu[:, 0].mean().item()\n",
    "    pred_mean_b = predicted_AB_cpu[:, 1].mean().item()\n",
    "    pred_std_a = predicted_AB_cpu[:, 0].std().item()\n",
    "    pred_std_b = predicted_AB_cpu[:, 1].std().item()\n",
    "    \n",
    "    # MSE between prediction and ground truth\n",
    "    mse = ((predicted_AB_cpu - AB_batch_gt_cpu) ** 2).mean().item()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“Š INFERENCE METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nGround Truth:\")\n",
    "print(f\"  A channel: mean = {gt_mean_a:+.3f}, std = {gt_std_a:.3f}\")\n",
    "print(f\"  B channel: mean = {gt_mean_b:+.3f}, std = {gt_std_b:.3f}\")\n",
    "print(f\"\\nPrediction:\")\n",
    "print(f\"  A channel: mean = {pred_mean_a:+.3f}, std = {pred_std_a:.3f}\")\n",
    "print(f\"  B channel: mean = {pred_mean_b:+.3f}, std = {pred_std_b:.3f}\")\n",
    "print(f\"\\nError:\")\n",
    "print(f\"  MSE (AB channels): {mse:.4f}\")\n",
    "print(f\"\\nSaturation (std ratio):\")\n",
    "print(f\"  A channel: {pred_std_a / gt_std_a * 100:.1f}%\")\n",
    "print(f\"  B channel: {pred_std_b / gt_std_b * 100:.1f}%\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205647e1",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 10: Save Individual Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654bb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual images\n",
    "print(\"Saving individual results...\")\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    # Grayscale\n",
    "    L_gray = L_batch_cpu[i, 0].numpy()\n",
    "    axes[0].imshow(L_gray, cmap='gray')\n",
    "    axes[0].set_title('Input', fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    rgb_gt = lab_to_rgb(L_batch_cpu[i], AB_batch_gt_cpu[i])\n",
    "    axes[1].imshow(rgb_gt)\n",
    "    axes[1].set_title('Ground Truth', fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    rgb_pred = lab_to_rgb(L_batch_cpu[i], predicted_AB_cpu[i])\n",
    "    axes[2].imshow(rgb_pred)\n",
    "    axes[2].set_title('Prediction', fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(OUTPUT_DIR, f'result_{i+1:02d}.png')\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"âœ… Saved {NUM_SAMPLES} individual results to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59ec6d",
   "metadata": {},
   "source": [
    "## ðŸ” Step 11: Upscale Results \n",
    "\n",
    "Upscale colorized images from 128Ã—128 to higher resolution for better viewing quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc291e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "from skimage.color import rgb2lab\n",
    "\n",
    "# Configuration\n",
    "UPSCALE_SIZE = 1024  # Target resolution (1024x1024)\n",
    "NUM_UPSCALE_SAMPLES = min(8, NUM_SAMPLES)  # How many to upscale and save\n",
    "\n",
    "print(f\"Upscaling {NUM_UPSCALE_SAMPLES} images from 128Ã—128 to {UPSCALE_SIZE}Ã—{UPSCALE_SIZE}...\")\n",
    "print(\"Using smart upscaling: preserving full-resolution L channel + upscaling AB only\")\n",
    "\n",
    "# Create upscaled directory\n",
    "upscaled_dir = os.path.join(OUTPUT_DIR, \"upscaled\")\n",
    "os.makedirs(upscaled_dir, exist_ok=True)\n",
    "\n",
    "def smart_upscale(L_tensor_128, predicted_AB_128, target_size=1024):\n",
    "    \"\"\"\n",
    "    Smart upscaling that preserves detail by:\n",
    "    1. Keeping full-resolution L channel from original 128x128\n",
    "    2. Upscaling only AB channels using high-quality interpolation\n",
    "    3. Combining full-res L with upscaled AB\n",
    "    \n",
    "    Args:\n",
    "        L_tensor_128: (1, 128, 128) L channel normalized [0, 1]\n",
    "        predicted_AB_128: (2, 128, 128) AB channels normalized [-1, 1]\n",
    "        target_size: Output resolution\n",
    "    \n",
    "    Returns:\n",
    "        RGB image as PIL Image at target_size Ã— target_size\n",
    "    \"\"\"\n",
    "    # Get L channel at 128x128 (we'll upscale this too for consistency)\n",
    "    L_128 = L_tensor_128.numpy() * 100.0  # Denormalize to [0, 100]\n",
    "    \n",
    "    # Get predicted AB at 128x128\n",
    "    predicted_AB = predicted_AB_128.numpy()  # (2, 128, 128)\n",
    "    predicted_AB = predicted_AB.transpose(1, 2, 0)  # (128, 128, 2)\n",
    "    \n",
    "    # Denormalize AB channels: [-1, 1] -> [-128, 128]\n",
    "    predicted_AB_denorm = predicted_AB * 128.0\n",
    "    \n",
    "    # Upscale L channel using high-quality LANCZOS interpolation\n",
    "    L_pil = Image.fromarray(L_128[0].astype(np.float32), mode='F')\n",
    "    L_upscaled = L_pil.resize((target_size, target_size), Image.Resampling.LANCZOS)\n",
    "    L_upscaled_array = np.array(L_upscaled)\n",
    "    \n",
    "    # Upscale AB channels separately using LANCZOS\n",
    "    a_channel_pil = Image.fromarray(predicted_AB_denorm[:, :, 0].astype(np.float32), mode='F')\n",
    "    b_channel_pil = Image.fromarray(predicted_AB_denorm[:, :, 1].astype(np.float32), mode='F')\n",
    "    \n",
    "    a_channel_upscaled = a_channel_pil.resize((target_size, target_size), Image.Resampling.LANCZOS)\n",
    "    b_channel_upscaled = b_channel_pil.resize((target_size, target_size), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    a_channel_full = np.array(a_channel_upscaled)\n",
    "    b_channel_full = np.array(b_channel_upscaled)\n",
    "    \n",
    "    # Combine full-resolution L with upscaled AB\n",
    "    result_lab = np.zeros((target_size, target_size, 3), dtype=np.float64)\n",
    "    result_lab[:, :, 0] = L_upscaled_array  # Upscaled L channel\n",
    "    result_lab[:, :, 1] = a_channel_full     # Upscaled A channel\n",
    "    result_lab[:, :, 2] = b_channel_full     # Upscaled B channel\n",
    "    \n",
    "    # Convert LAB to RGB\n",
    "    result_rgb = lab2rgb(result_lab)\n",
    "    result_rgb = np.clip(result_rgb, 0, 1)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    result_pil = Image.fromarray((result_rgb * 255).astype(np.uint8))\n",
    "    \n",
    "    return result_pil\n",
    "\n",
    "# Upscale and save results\n",
    "fig, axes = plt.subplots(3, NUM_UPSCALE_SAMPLES, figsize=(NUM_UPSCALE_SAMPLES * 3, 9))\n",
    "\n",
    "for i in range(NUM_UPSCALE_SAMPLES):\n",
    "    # Grayscale upscaled\n",
    "    L_gray = L_batch_cpu[i, 0].numpy()\n",
    "    L_upscaled_pil = Image.fromarray((L_gray * 255).astype(np.uint8), mode='L')\n",
    "    L_upscaled_pil = L_upscaled_pil.resize((UPSCALE_SIZE, UPSCALE_SIZE), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    axes[0, i].imshow(np.array(L_upscaled_pil), cmap='gray')\n",
    "    axes[0, i].set_title(f'Input {UPSCALE_SIZE}Ã—{UPSCALE_SIZE}', fontweight='bold', fontsize=9)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Ground truth - smart upscale\n",
    "    gt_upscaled = smart_upscale(L_batch_cpu[i], AB_batch_gt_cpu[i], UPSCALE_SIZE)\n",
    "    axes[1, i].imshow(gt_upscaled)\n",
    "    axes[1, i].set_title(f'GT {UPSCALE_SIZE}Ã—{UPSCALE_SIZE}', fontweight='bold', fontsize=9)\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Prediction - smart upscale\n",
    "    pred_upscaled = smart_upscale(L_batch_cpu[i], predicted_AB_cpu[i], UPSCALE_SIZE)\n",
    "    axes[2, i].imshow(pred_upscaled)\n",
    "    axes[2, i].set_title(f'Predicted {UPSCALE_SIZE}Ã—{UPSCALE_SIZE}', fontweight='bold', fontsize=9)\n",
    "    axes[2, i].axis('off')\n",
    "    \n",
    "    # Save individual upscaled images\n",
    "    L_upscaled_pil.save(os.path.join(upscaled_dir, f'upscaled_{i+1:02d}_input.png'))\n",
    "    gt_upscaled.save(os.path.join(upscaled_dir, f'upscaled_{i+1:02d}_groundtruth.png'))\n",
    "    pred_upscaled.save(os.path.join(upscaled_dir, f'upscaled_{i+1:02d}_prediction.png'))\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_path = os.path.join(upscaled_dir, 'upscaled_comparison.png')\n",
    "plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"âœ… Saved upscaled comparison to {comparison_path}\")\n",
    "print(f\"âœ… Saved {NUM_UPSCALE_SAMPLES * 3} individual upscaled images to {upscaled_dir}/\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸ“Š Upscaling Summary:\")\n",
    "print(f\"  Original: 128Ã—128 pixels\")\n",
    "print(f\"  Upscaled: {UPSCALE_SIZE}Ã—{UPSCALE_SIZE} pixels (8x resolution)\")\n",
    "print(f\"  Method: Smart upscaling (LANCZOS interpolation on L and AB separately)\")\n",
    "print(f\"  Detail preservation: Full-resolution L channel + upscaled AB channels\")\n",
    "print(f\"  Output: {upscaled_dir}/\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
